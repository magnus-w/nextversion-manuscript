# Vad ska vi lära maskinen idag

**Vad ska vi lära maskinen idag?**

När du läser eller hör någon prata om AI så är det idag _maskininlärning_ de menar. Och förutom att det är vårt tredje “M” så är det också en effekt av de första två. Ett barn av sin tid, helt enkelt.

Praktiska tillämpningar av Artificiell Intelligens tog fart i slutet av 1950-talet och forskare som Daniel Dennett brukar säga att vi nu lever i/ser den tredje hajpen av AI. Kanske beror det på att maskininlärning givit datorerna sådan förmågor att vi inte har något annat att jämföra dem med. Bara det att kunna prata med sin stereoanläggning och be den spela låtar eller köra en musikquiz! Men det betyder inte att stereon blivit vid medvetande, utan att mönsterigenkänning, algoritmer och molntjänster samverkar på ett nytt sätt. Och det här är ett “M” som ännu bara är i sin linda. Du kommer att höra om många organisationer som skaffar sig AI:s och som bygger nya tjänster och funktioner med hjälp av dem.

Molnet är en dubbel förutsättning för maskininlärning och det började egentligen i helt andra ändar. När Apple introducerade den första iPhonen var en av de största skillnaderna jämfört med tidigare smartphones att a\) all interaktion skedde med hjälp av berörning av skärmen och att den b\) inte hade ett fysiskt tangentbord. Så fort ditt finger klickade på en funktion som ville att du skulle skriva text, fälldes ett virtuellt tangentbord ut på skärmen. Den stora fördelen var att i princip hela telefonen då kunde bara skärm och ge mer plats åt bilder, film, spel och sajter. För första gången blev det praktiskt möjligt att surfa på en mobiltelefon, något de flest av oss glömt bort eller tar för givet. En annan fördel med ett virtuellt tangentbord förutom att de väldigt enkelt kunde fällas bort, var att det var mjukvara. Det blev med andra ord lätt att visa upp olika bokstäver på tangenterna beroende på vilket språk användaren ville skriva på. Något som hade varit ett stort problem på tidigare smartphones, som hade fått förlita sig på funktionstangenter och olika modellversioner av telefonerna.

Men det fanns en nackdel.

Ett fysiskt tangentbord ger taktil feedback --- du känner när du tryckt ned en tangent och du känner tangenterna bredvid. Även om de var små var de lättare att skriva på, lättare att skriva rätt på. Åtminstone i det virtuella tangentbordets barndom. För att hjälpa oss att skriva rätt skaffade Apple hjälp från mer mjukvara. Ericssons T9 --- ett program som gav användaren ett antal olika alternativ när man börjat skriva några bokstäver --- hade redan hjälp en generation mobilanvändare. Men Apple gick ett steg längre och lät sitt program också gissa vad märkliga bokstavskombinationer kunde betyda. Om min iPhone ser att jag skriver “nsfnyd” så kommer den att gissa att alla bokstäver utom “n” beror på att jag har tjocka fingrar och egentligen menade “Magnus”. Dels för att _nsfnyd_ inte är ett ord i det svenska språket --- än --- och dels för att jag heter Magnus, vilket telefonen redan vet.

Det här ledde till en märklig upplevelse för de som började använda en iPhone för första gången, särskilt om de använt fysiska tangentbord tidigare. Försökte man vara noggrann och skriva långsamt blev det mer fel än om man bara skrev på! För då gick korrigeringen igång. Samma korrigering skedde så klart med andra ord i mjukvarans ordlista, men sedan kom själva nymodigheten. Tangentbordet kunde lära sig nya ord, sådana som den felaktigt försökt korrigera, om du talade om hur det egentligen skulle vara. Plus, eftersom alla iPhones var uppkopplade till ett snabbt Internet via 3G, så kunde Apple samla “tangentbordsbeteenden” och “ordbeteenden” från alla användare. Dina feltryck och de nya orden i din ordlista kunde med andra ord hjälpa mjukvaran i andras telefoner att korrigera orden. En funktion som var viktigare än man kan tro, eftersom de officiella ordlistor över språk som mjukvara kommer laddad med från början sällan innehåller ord som är dagligt tal, daglig skrift.

Hela kedjan är ett bra fint exempel på hur en förbättring leder till flera, steg för steg. En smartphone kan vara en enda stor stor skärm om den inte behöver ett fysiskt tangentbord. Och touch är också en mer flexibel interaktionsmetod än tangenter: Den kan i princip vara olika i olika appar. Men touch hade också nackdelar, särskilt vid lanseringen då det var nytt.

Att ha en ordlista som kunde korrigera ord medan man skrev var inte nytt. Men att korrigera baserat på så “stora missar/bokstavsmissstag” var det. Att den förinstallerade ordlistan kunde utökas av användaren var inte nytt. Men att den utökades både av ord och av bokstavsmisstag från _andra användare_ var det. Och att denna uppdatering skedde automatiskt i bakgrunden, utan att användaren behövde göra något var också nytt. Den “totala ordlistan”

Här ser vi alltså kombinationen av tre saker: 1. En mjukvara som är självlärande. 2. Som skickar data att lära av till och hämtar från molnet. 3. Som alltid har delar av sin data i molnet, inklusive uppdateringar av sig själv.

Som “intelligens” är det inte mycket att komma med, vare sig artificiell eller inte. \(Även om vi brukar få för oss att personer som stabar bättre är smartare än andra.\) Men som _automatisering_ av ett antal flöden, varav några är helt nya och andra aldrig varit automatiserade är det en ordentlig bedrift. Att det tog både användare och branschexperter så pass länge att förstå att det var så här det gick till är också intressant. I början var det många som skrev om hur “märkligt bra” det virtuella tangentbordet var. Efter att man använt det ett tag blev det till och med bättre än ett fysiskt! Hur gick det till?

Och just _automatisering_ är det vi använt de förstadier\(annat ord?\) till artificiell intelligens som vi hittills lyckas utveckla. Den allra första var när vi lät elektronik automatisera hissar, vilket är lite mer komplicerat än många av oss tänker på när vi kliver in i en. \(Det är en sak om det bara finns en hiss, men i ett kontorshus eller hotell med flera behöver de hålla kolla på var de är för att skicka rätt hiss just till den våning du står och trycker på!\)

Samtidigt som, framför allt Apple och Google, började använda molnet som ett sätt att distribuera data som skyndade på upplärningen/utbildningen/skolningen av olika program, hände något annat. Smartphones och surfplattor var utmärkta för underhållning --- till exempel kunde de agera spelkonsoll och kontroll på samma gång. Vilket ledde till snabb utveckling av mobila grafikkort och snabbare och bättre spel. Snart kunde vi alla ha motsvarande Playstation- och XBox-spel i fickan. Detta drev i sin tur på spelutveckling på de traditionella plattformarna. Som behövde ha ännu mer fantastisk grafik för att konkurrera med mobiler och plattor. Och den utveckling av grafikkort som det ledde till hade en oplanerad sidoeffekt. Det visade sig nämligen att grafikprocessorer --- CGU:s --- är mycket bättre lämpade än vanliga dataprocessorer --- CPU:s --- att bearbeta maskininlärning. Något med grafikprocessorns struktur för att hantera bilder, hantera massor av pixlar, stämde överens med hur _deep learning_ fungerar. Det månggrenade beslutsträdet som växer och förgrenar sig själv när vi ber en algoritm ta reda på hur den borde se ut för att kunna se skillnad på katter och allt annat i en bild.

Lätt till detta att när vi fått smartphones i våra händer så såg drömmen från Star Trek ut att kunna bli sann. Där har Kapten Kirk en tingest han kan bära med sig i handen som översätter vad aliens säger. Google, som redan hade lanserat Google Translate \(kolla årtal\), tänkte sig att om du kunde prata in texten i den tjänsten och den kunde läsa upp vad du sagt på ett annat språk så skulle man vara nära. I princip behövdes “bara” förmågan att omvandla det du sa till text och sen att omvandla text till tal. Google Translate fortsatte göra översättningen och där hade Google också gått ifrån att “leta i lexikon” till maskininlärning. Det var nödvändigt när användare började skriva in hela meningar, eller klistra in hela stycken av text de ville ha översatta. Så istället för att slå efter ord i ett lexikon jämförde Googles algoritm textstycken med andra som redan fanns på flera språk. Och eftersom allt detta skedde i molnet, hade du omedelbara tillgång till de förbättringar som Translate på detta sätt skapade själv varje gång vi interagerade med tjänsten. \(Du kan själv ganska lätt se dessa förbättringar genom att peta in något lite mer komplicerat i Google Translate och se vad du får för resultat. Och sen göra om samma övning någon månad senare.\) Med hjälp av framsteg inom _natural language recognition_ och maskininlärning som fort fart av grafikkorten blev den här “automatiserad tolktjänsten” snabbt bättre.

Samtidigt, runt 2010, hade ett litet företag i San Francisco släppt en app till iPhonen som byggde på en liknande idé. Fast istället för att översätta mellan språk översatte den tal till interaktioner/händelser på telefonen. De hade helt enkelt byggt en röstassistent och företaget hette Siri. Ursprungligen var det en spinoff från ett projekt hos SRI International Artificial Intelligence Center, därav namnet. Man kombinerade maskininlärning med röstigenkänning från Nuance Communications och appen fanns bara på App Store i två månader. Sedan köpte Apple företaget och integrerade funktion i iOS/sitt operativsystem tills det var dags att släppa iPhone 4s, som blev den första smartphonen med en inbyggd röstassistent.

Nu visar siffror att 73% av alla mellan 25 och 40 år använder röstassistenten på sin smartphone dagligen var att googla, sätta på musik, kolla vädret och allt annat som telefonen kan göra. Även om det kanske inte verkar som en jättenödvändig funktion är möjligheten att säga “Ring mamma” istället för att trycka på telefonen av stor betydelse för till exempel trafiksäkerhet. Eller för användare som har en funktionsnedsättning. I och med att röstassistenten också kan prata tillbaka kan nu blinda använda smartphones på ett helt annat sätt.

Och, även om det kan verka som att vi tjatar, Siri, Google Assistant och Amazons Alexa blir hela tiden bättre och bättre eftersom de lär sig själva från alla sina förehavanden med _alla_ användare.

Precis som när iPaden kom, var det många experter som inte trodde på Alexa Echo. Men bara i Amerika finns det nu HomePods, Google Home och Alexa i 50 miljoner hem. Det är just nu den snabbast växande sortens enhet i försäljningsvolym. Du kommer ihåg hur vi pratade att många bedömare trodde att mobilen skulle ge oss en värld där vi bara använda smartphones till allting? Nu är det alltså redan var femte hushåll i USA som pratar med sin högtalare och ber om en väderrapport, en sång eller ett recept, istället för att använda sin telefon.

En av de största anledningarna till att det går så här fort är inte utvecklingen inom maskinlärningen i sig. Utan att alla dessa nya förmågor som så många andra tillgängliggörs som moduler i molnet.

Du kunde prata med din dator redan i slutet av 1990-talet om du ville ha ett hjälpmedel för en funktionsnedsättning eller bara diktera istället för att skriva med tangentbordet. Men de mjukvaran kunde vad den kunde när du installerade den och blev inte märkbart bättre. Nya versioner kom kanske en gång om året och dem var du tvungna att köpa och installera.

Nu är taligenkänning och talsyntes tjänster som finns i molnet och som är separata moduler från översättning eller till exempel vädret. Det är genom att sätta ihop flera sådana moduler vi kan bygga tjänster som felkaktigt/slentrianmässigt kallas Artificiell Intelligens. Och det som är intressant när du ska öka farten på din digitala utveckling och på förbättringen av kundupplevelsen är hur du kan använda maskininlärning för att just _automatisera_ flöden.

Chatbotar är ett sådant exempel. De kan automatisera tidiga delar i kedjan för kundtjänst. Och redan nu inser vi att namnet chatbot är förlegat, eftersom du kan välja att den går att prata med istället bara genom att koppla till sådana moduler i molnet.

Ett roligt exempel på detta är när Google Assistant lanserades på svenska hösten 2018. Ganska snart kunde man läsa nyheten i branschpress vilka svenska företag som var snabb ute och redan hade byggt rösttjänster som fungerade på svenska i Google Home-högtalarna. I verkligheten hade de redan byggt chatbotar, men i och med att de hade byggt dem på rätt sätt --- det vill säga modulärt och i molnet --- så kunde de i princip med en knapptryckning slå på _Röst_ och _Svenska_ i Google Cloud.

**Hjärnor på kran istället för att vara datadriven**

Med hjälp av Googles _Tensorflow_ och _DialogueFlow_ eller IBM:s _Watson,_ kan alla som vill nu “prenumerera” på maskininlärning och behöver inte bygga den själv. Med andra ord ytterligare ett exempel på moduler.

Alltså finns det ingen anledning att vänta. Du och din organisation kan istället testa och sätta ihop en tjänst baserad på maskininlärning som automatiserar något led i flödet mellan er och era kunder. Det kan vara enkla saker som vad man får se när man kommer in på er sajt. Det finns ju ingen anledning att visa alla användare samma sak, när vi vet att de har olika behov och de visar dessutom vilka de är genom sina digitala beteenden.

På samma sätt som med att ha sina servrar i molnet betyder det här att företag inte behöver göra samma stora investeringar i förväg som det tidigare innebar att bygga sin egen “artificiella intelligens”. Istället prenumererar du och betalar efter hur mycket den används. Och allt detta har gått mycket snabbt. \(Konstigt vore väl annars när vi har en snabb förändringstakt?\) Så snabbt att företag så sent som för tre-fyra år sedan pratade om att bygga sina egna “rekommendationsmotorer”.

Vilket osökt leder oss in på en glosa vi redan tagit upp --- _Big Data._ Tanken att om du bara samlade tillräckligt mycket data, enorma mängder alltså, så skulle du kunna använda dem till att utveckla nya affärer och tjänster. Det stora problemet var hur du skulle hitta mönster i dina stora datasjöar och hur du skulle analysera dem.

Maskininlärning både löser problemet och ändrar frågeställningen.

Den hittar de mönster som användarna och kunderna själva skapar, istället för att du ska gruppera in dem i påhittade grupper. Samtidigt som den kan använda data i realtid, så du behöver inte samla lika stora sjöar längre.

Egentligen har de flesta företag redan mer data än vad de behöver. Framför allt för att de i dagsläget inte använder ens en bråkdel av den till automatisering, beslutsfattande eller bättre kundupplevelser. Och det är bara kostsamt att samla på sig stora mängder data som man inte använder. Vad som är väldigt lätt att glömma bort är att den digitala världen vi lever i skapar ny data hela tiden. Det är som en pågående beteendeström. Just nu finns det till exempel besökte på er sajt som gör saker och varje scrolling, varje klick kan du se --- om du bara vill.

Att göra något av det/åt det beteendet just som det sker är värt så oerhört mycket mer än att bara _spara_ det som data i en stor sjö. Det är ungefär lika vettigt som att studera vad en kund gör i en butik, se hur hen går fram till en hylla med produkter och verkar leta efter något, men inte hitta det. Och istället för att då gå fram och erbjuda sig att hjälpa till, så antecknar du bara beteendet för att använda senare när du samlat många.

Till datasjöarnas försvar kan vi säga att de som höll på att fylla dem var så upptagna med det att de inte tittade på maskininlärning. Ytterligare ett exempel på vad som händer när man arbetar i silos. Då vill dataanalytikerna samla data och analysera dem på sin avdelning, medan en annan avdelning håller på med något som skulle kunna hjälpa dem, med det vet de inte om.

Analytikerna pratade då om att organisationer behövde bli _datadrivna_ och det var kanske rätt i sak, det var bara själva praktiken de inte hade fått till på ett nutida sätt. Det som var bra med resonemanget var att företagets beslut inte skulle bygga på magkänsla och enstaka personers idéer om vad som var rätt och fel, utan baseras på kunddata. Men precis som i exemplet ovan från butiken så ökar vi inte utvecklingsfarten genom att först samla in, sen analysera och sen agera.

Vad vi istället kan göra med maskininlärningens hjälp är att agera i realtid. Eftersom vi med hjälp av den _automatiserar_ både “avlyssnandet” av beteenden och analysen, kan vi också automatisera stora delar av agerandet, av vad vi de facto gör för att anpassa kundupplevelsen till respektive kund. När kunden står och letar på hyllan kan vi automatiskt skicka fram en automatisk expedit som ger kunden hjälp.

Vi kan inte nog påpeka hur viktig denna aspekt av en modern kundupplevelse är. Plus, för att gå tillbaka till tjatet, det visar hur viktigt det är att ha ett öppet och silofritt sinne för att ta vara på möjligheterna som utvecklingstakten ger oss. På ett sätt är det nämligen inte konstigt att analytikerna och “datasjöavdelningen” inte är de som sett hur maskinerna kan hjälpa oss med detta. Om du skapar realtidsflöden så har du ju gjort en genväg förbi analytikerna och datasjöarna. De har blivit onödiga.

Istället behöver du realtidsflöden av data och en maskin som både avlyssnar dem och är duktigt på att lära sig. För här kommer ett varv till: När maskininlärningen gjort en rekommendation till kunden, baserat på hens beteende, så lyssnar den på vad den rekommendationen åstadkommer. På vilket sätt ändras kundens beteende? Kopplingen mellan beteende-rekommendation-beteende leder i sin tur till en ny rekommendation; en ny anpassning av kundupplevelsen. Och så vidare, och så vidare.

Men betyder det att det inte längre finns något behov av handpåläggning? Behöver människor inte göra något manuellt för att sätta fart på förbättringen av kundupplevelsen?

Jo, massor! Och hur det går till ska vi ägna resten av boken åt.

Det finns också massor av andra - mönstret, en modul i molnet istället för att bygga allt själv, varje applikation som en egen ö.

